---
key: 20190720
title: 容器实践线路图
tags: Container, Docker, Practice
published: false
---

随着容器技术越来越火热，各种大会上标杆企业分享容器化收益，带动其他还未实施容器的企业也在考虑实施容器化。不过真要在自己企业实践容器的时候，会认识到容器化不是一个简单工程，甚至会有一种茫然不知从何入手的感觉。

本文总结了通用的企业容器化实施线路图，主要针对企业有存量系统改造为容器，或者部分新开发的系统使用容器技术的场景。不包含企业系统从0开始全新构建的场景，这种场景相对简单。<!--more-->

## 容器实践路线图

企业着手实践容器的路线，建议从3个维度评估，然后根据评估结果落地实施。3个评估维度为：商业目标，技术选型，团队配合。

1. 商业目标是重中之重，需要回答为何要容器化，这个也是牵引团队在容器实践路上不断前行的动力，是遇到问题是解决问题的方向指引，最重要的是让决策者认同商业目标，上下目标对齐才好办事。

2. 商业目标确定之后，需要确定容器相关的技术选型，容器是一种轻量化的虚拟化技术，与传统虚拟机比较有优点也有缺点，要找出这些差异点识别出对基础设施与应用的影响，提前识别风险并采取应对措施。

3. 技术选型明确之后，在公司或部门内部推广与评审，让开发人员、架构师、测试人员、运维人员相关人员与团队理解与认同方案，听取他们意见，他们是直接使用容器的客户，不要让他们有抱怨。

4. 最后是落地策略，一般是选取一些辅助业务先试点，在实践过程中不断总结经验。

## 商业目标

容器技术是以应用为中心的轻量级虚拟化技术，而传统的Xen与KVM是以资源为中心的虚拟化技术，这是两者的本质差异。以应用为中心是容器技术演进的指导原则，正是在这个原则指导下，容器技术相对于传统虚拟化有几个特点：打包既部署、镜像分层、应用资源调度。

- 打包既部署：打包即部署是指在容器镜像制作过程包含了传统软件包部署的过程（安装依赖的操作系统库或工具、创建用户、创建运行目录、解压、设置文件权限等等），这么做的好处是把应用及其依赖封装到了一个相对封闭的环境，减少了应用对外部环境的依赖，增强了应用在各种不同环境下的行为一致性，同时也减少了应用部署时间。
- 镜像分层：容器镜像包是分层结构，同一个主机上的镜像层是可以在多个容器之间共享的，这个机制可以极大减少镜像更新时候拉取镜像包的时间，通常应用程序更新升级都只是更新业务层（如Java程序的jar包），而镜像中的操作系统Lib层、运行时（如Jre）层等文件不会频繁更新。因此新版本镜像实质有变化的只有很小的一部分，在更新升级时候也只会从镜像仓库拉取很小的文件，所以速度很快。
- 应用资源调度：资源（计算/存储/网络）都是以应用为中心的，**中心**体现在资源分配是按照应用粒度分配资源、资源随应用迁移。

基于上述容器技术特点，可以推导出容器技术的3大使用场景：CI/CD、提升资源利用率、弹性伸缩。这3个使用场景自然推导出通用的商业层面收益：CI/CD提升研发效率、提升资源利用率降低成本、按需弹性伸缩在体验与成本之间达成平衡。

当然，除了商业目标之外，可能还有其他一些考虑因素，如基于容器技术实现计算任务调度平台、保持团队技术先进性等。

### CI/CD提升研发效率

### 提升资源利用率

### 按需弹性伸缩在体验与成本之间达成平衡

### 基于容器技术实现计算调度平台

## 技术选型

容器技术是属于基础设施范围，但是与传统虚拟化技术（Xen/KVM）比较，容器技术是应用虚拟化，不是纯粹的资源虚拟化，与传统虚拟化存在差异。在容器技术选型时候，需要结合当前团队在应用管理与资源管理的现状，对照容器技术与虚拟化技术的差异，选择最合适的容器技术栈。

### 容器与传统虚拟化差异

#### 什么是容器

在讨论容器与虚拟化差异之前，我们有必要明确一下，从技术角度来看，到底什么是容器(container)。在讨论容器的时候，会关联到很多技术，```LXC```, ```cgroups```, ```namespace```, ```aufs``` 等等，容器与这些技术之间是什么关系？

通俗层面来讲，可以这么定义容器：

```容器(container) ~= 应用打包(build) + 应用分发(ship) + 应用运行/资源隔离(run)```。

build-ship-run 的内容都被定义到了[OCI规范](https://www.opencontainers.org/)中，因此也可以这么定义容器：

 ```容器(container) == OCI规范```

OCI规范包含两部分，[镜像规范](http://www.github.com/opencontainers/runtime-spec)与[运行时规范](http://www.github.com/opencontainers/image-spec)。简要的说，要实现一个OCI的规范，需要能够下载镜像并解压镜像到文件系统上组成成一个文件目录结构，运行时工具能够理解这个目录结构并基于此目录结构管理（创建/启动/停止/删除）进程。

而我们所说的docker，是一个管理应用 build-ship-run 的工具，同时实现了OCI规范。任何一个公司或组织，都可通过参考OCI规范实现"docker'"，实际上也有不少公司这么做了，只是没有开放出来在内部使用。

对于不同的操作系统（Linux/Windows），OCI规范的实现不同，当前docker的实现，支持Windows与Linux与MacOS操作系统。当前使用最广的是Linux系统，OCI的实现，在Linux上使用的主要技术：

- chroot: 通过分层文件系统堆叠出容器进程的rootfs，然后通过```chroot```设置容器进程的根文件系统为堆叠出的rootfs。
- cgroups: 通过cgroups技术隔离容器进程的cpu/内存资源。
- namesapce: 通过```pid```, ```uts```, ```mount```, ```network```, ```user``` namesapce 分别隔离容器进程的进程ID，时间，文件系统挂载，网络，用户资源。
- 网络虚拟化: 容器进程被放置到独立的网络命名空间，通过Linux网络虚拟化```veth```, ```macvlan```, ```bridge```等技术连接主机网络与容器虚拟网络。
- 存储驱动: 本地文件系统，使用容器镜像分层文件堆叠的各种实现驱动，当前推荐的是```overlay2```。

广义的容器还包含容器编排，即当下很火热的Kubernetes。Kubernetes为了把控容器调度的生态，发布了[CRI规范](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/)，通过CRI规范解耦Kubelet与容器，只要实现了CRI接口，都可以与Kubelet交互，从而被Kubernetes调度。OCI规范的容器实现与CRI标准接口对接的实现是[CRI-O](https://github.com/cri-o/cri-o)。

#### 容器与虚拟机差异对比

参照什么是容器里的说明，可以概括出容器与虚拟机的差异为2点：应用打包与分发的差异，应用资源隔离的差异。当然，导致这两点差异的根基是容器是以应用为中心来设计的，而虚拟化是以资源为中心来设计的，本文对比容器与虚拟机的差异，更多的是站在应用视角来对比。
从3个方面对比差异：资源隔离，应用打包与分发，延伸的日志/监控/DFX差异。

##### 1.资源隔离

1. 隔离机制差异

|     | 容器 | 虚拟化  |
| --- | ---  | --- |
| mem/cpu |  cgroup, 使用时候设定 ```require``` 与 ```limit``` 值  |  QEMU, KVM  |
| network |  Linux网络虚拟化技术(veth,tap,bridge,macvlan,ipvlan), 跨虚拟机或出公网访问:SNAT/DNAT, service转发:iptables/ipvs, SR-IOV  | Linux网络虚拟化技术(veth,tap,bridge,macvlan,ipvlan), QEMU, SR-IOV   |
| storage |  本地存储: 容器存储驱动  |  本地存储：virtio-blk  |

2. 差异引入问题与实践建议

- 应用程序未适配 ```cgroup``` 的内存隔离导致问题: 典型的是 ```JVM``` 虚拟机，在 ```JVM``` 启动时候会根据系统内存自动设置 ```MaxHeapSize``` 值，通常是系统内存的1/4，但是 ```JVM``` 并未考虑 ```cgroup``` 场景，读系统内存时候任然读取主机的内存来设置 ```MaxHeapSize```，这样会导致内存超过 ```cgroup``` 限制从而导致进程被 ```kill``` 。问题详细阐述与解决建议参考[Java inside docker: What you must know to not FAIL](https://developers.redhat.com/blog/2017/03/14/java-inside-docker/)。
- 多次网络虚拟化问题: 如果在虚拟机内使用容器，会多一层网络虚拟化，并加入了SNAT/DNAT技术, iptables/ipvs技术，对网络吞吐量与时延都有影响（具体依赖容器网络方案），对问题定位复杂度变高，同时还需要注意网络内核参数调优。

```
典型的网络调优参数有：
    转发表大小 ```/proc/sys/net/netfilter/nf_conntrack_max``` 

使用iptables 作为service转发实现的时候，在转发规则较多的时候，iptables更新由于需要全量更新导致非常耗时，建议使用ipvs。详细参考[华为云在 K8S 大规模场景下的 Service 性能优化实践](https://zhuanlan.zhihu.com/p/37230013)。
```

- 容器IP地址频繁变化不固定，周边系统需要协调适配，包括基于IP地址的白名单或防火墙控制策略需要调整，CMDB记录的应用IP地址需要适配动态修改或者使用服务明替代IP地址。

- 存储驱动带来的性能损耗: 容器本地文件系统是通过联合文件系统方式堆叠出来的，当前主推与默认提供的是```overlay2```驱动，这种模式应用写本地文件系统文件或修改已有文件，使用Copy-On-Write方式，也就是会先拷贝源文件到可写层然后修改，如果这种操作非常频繁，建议使用 ```volume``` 方式。

##### 2.应用打包与分发

1. 应用打包/分发/调度差异

|     | 容器 | 虚拟化  |
| --- | ---  | --- |
| 打包 | 打包既部署  |  一般不会把应用程序与虚拟机打包在一起，通过部署系统部署应用   |
| 分发 | 使用镜像仓库存储与分发 | 使用文件存储 |
| 调度运行 | 使用K8S亲和/反亲和调度策略 | 使用部署系统的调度能力 |

2. 差异引入问题与实践建议

- 部署提前到构建阶段，应用需要支持动态配置与静态程序分离；如果在传统部署脚本中依赖外部动态配置，这部分需要做一些调整。
- 打包格式发生变化，制作容器镜像需要注意安全/效率因素，可参考[Dockerfile最佳实践](/2019/06/26/Dockerfile最佳实践.html)
- 容器镜像存储与分发是按layer来组织的，镜像在传输过程中放篡改的方式是传统软件包有差异。


##### 3.监控/日志/DFX

1. 差异

|     | 容器 | 虚拟化  |
| --- | ---  | --- |
| 监控 | cpu/mem的资源上限是cgroup定义的；containerd/shim/docker-daemon等进程的监控  | 传统进程监控  |
| 日志采集 | stdout/stderr日志采集方式变化；日志持久化需要挂载到volume；进程会被随机调度到其他节点导致日志需要实时采集否则分散很难定位 | 传统日志采集 |
| 问题定位 | 进程down之后自动拉起会导致问题定位现场丢失；无法停止进程来定位问题因为停止即删除实例 | 传统问题定位手段 |
| 安全 |  |  |
| 进程故障检测 |  |  |

2. 差异引入问题实践与建议

- 使用成熟的监控工具，运行在docker中的应用使用cadvisor+prometheus实现采集与警报，cadvisor中预置了常用的监控指标项
- 对于docker管理进程（containerd/shim/docker-daemon）也需要一并监控
- 使用成熟的日志采集工具，如果已有日志采集Agent，则可以考虑将日志文件挂载到volume后由Agent采集；需要注意的是stderr/stdout输出也要一并采集
- 如果希望容器内应用进程退出后保留现场定位问题，则可以将```Pod```的```restartPolicy```设置为```never```，进程退出后进程文件都还保留着(/var/lib/docker/containers)。但是这么做的话需要进程没有及时恢复，会影响业务，需要自己实现进程重拉起。

### 典型的几种应用虚拟化技术

LXC，UniKernel（LibOS），虚拟机容器。

### 选择合适的容器技术栈

## 团队配合

## 落地策略

## Reference

[DOCKER vs LXC vs VIRTUAL MACHINES](https://www.linkedin.com/pulse/docker-vs-lxc-virtual-machines-phucsi-nguyen/)

[Cgroup与LXC简介](https://blog.51cto.com/speakingbaicai/1359825)

[Introducing Container Runtime Interface (CRI) in Kubernetes](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/)

[frakti](https://github.com/kubernetes/frakti)

[rkt](https://github.com/rkt/rkt)

[appc-spec](https://github.com/appc/spec/)

[OCI 和 runc：容器标准化和 docker](https://cizixs.com/2017/11/05/oci-and-runc/)

[Linux 容器技术史话：从 chroot 到未来](https://linux.cn/article-6975-1.html)

[Linux Namespace和Cgroup](https://segmentfault.com/a/1190000009732550)

[Java inside docker: What you must know to not FAIL](https://developers.redhat.com/blog/2017/03/14/java-inside-docker/)

[QEMU,KVM及QEMU-KVM介绍](https://www.jianshu.com/p/4e893b5bfe81)

[kvm libvirt qemu实践系列(一)-kvm介绍](https://opengers.github.io/virtualization/kvm-libvirt-qemu-1/)

[KVM 介绍（4）：I/O 设备直接分配和 SR-IOV [KVM PCI/PCIe Pass-Through SR-IOV]](https://www.cnblogs.com/sammyliu/p/4548194.html)

[prometheus-book](https://yunlzheng.gitbook.io/prometheus-book/introduction)
